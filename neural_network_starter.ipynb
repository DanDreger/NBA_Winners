{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our dependencies\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "#  Import nba data\n",
    "nba_gamestats_df = pd.read_csv(\"Basketball_Data/curated_game_stats.csv\")\n",
    "print(nba_gamestats_df.head())\n",
    "\n",
    "nba_advancedgame_df = pd.read_csv(\"Basketball_Data/Advanced_Game_Stats_2001-2003.csv\")\n",
    "nba_advancedgame_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check types for nba game stats\n",
    "nba_gamestats_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check types for nba advanced stats\n",
    "nba_advancedgame_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test matching dates needed for merge\n",
    "nba_gamestats_df.loc[nba_gamestats_df[\"Date\"] == nba_advancedgame_df[\"Date\"][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop columns not needed, rename column\n",
    "nba_advancedgame_df = nba_advancedgame_df.drop([\"Result\", \"Rk\", \"FTr\",\t'3PAr',\t'eFG%.1',\t'FT/FGA',\t'ORtg.1',\t'FTr.1',\t'3PAr.1',\t'TS%.1',\t'eFG%.2',\t'FT/FGA.1'], axis=1)\n",
    "\n",
    "nba_advancedgame_df.rename({\n",
    "  \"Unnamed: 4\" : \"Home/Away\"\n",
    "}, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "nba_advancedgame_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop columns not needed\n",
    "nba_gamestats_df = nba_gamestats_df.drop([\"Unnamed: 0\"], axis=1)\n",
    "\n",
    "nba_gamestats_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_stats_df = pd.merge(nba_gamestats_df, nba_advancedgame_df, on=[\"Date\", \"Team\", \"Opp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform Dataframe\n",
    "complete_stats_df=complete_stats_df.astype({\n",
    "  'PTS' : 'float',\n",
    "  'Opponent_Points' : 'float',\n",
    "  'Home/Away': 'string',\n",
    "  'FG': 'float',\n",
    "  'FGA': 'float',\n",
    "  '2PA': 'float',\n",
    "  '3P': 'float',\n",
    "  '3PA': 'float',\n",
    "  'Team': 'string',\n",
    "  'Opp': 'string'\n",
    "})\n",
    "\n",
    "complete_stats_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add Column and convert home/away\n",
    "complete_stats_df[\"Pts Difference\"] =  complete_stats_df[\"PTS\"] - complete_stats_df[\"Opponent_Points\"]\n",
    "complete_stats_df['Home/Away'] = complete_stats_df['Home/Away'].replace(['@', ''], ['Away', 'Home'])\n",
    "complete_stats_df['Home/Away'].fillna(value=\"Home\", inplace=True)\n",
    "\n",
    "complete_stats_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_stats_df.drop([\"Date\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_points_df = complete_stats_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_stats_df.loc[complete_stats_df[\"Pts Difference\"] >= 0, \"Pts Difference\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_stats_df.loc[complete_stats_df[\"Pts Difference\"] < 0, \"Pts Difference\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_stats_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Model for Win/Loss Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split our preprocessed data into our features and target arrays\n",
    "y = complete_stats_df['Pts Difference']\n",
    "X = complete_stats_df.drop([\"Team\", \"Opp\",'Pts Difference'], axis= 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One Hot Encoding\n",
    "X = pd.get_dummies(X, columns=[\"Home/Away\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the preprocessed data into a training and testing dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StandardScaler instances\n",
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "# Fit the StandardScaler\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate Neural Network Model\n",
    "nn_model = tf.keras.models.Sequential()\n",
    "\n",
    "unit = 2 * X_train_scaled.shape[1]\n",
    "\n",
    "#Dense Layers\n",
    "nn_model.add(tf.keras.layers.Dense(units=unit, activation=\"relu\", input_dim=X_train_scaled.shape[1]))\n",
    "nn_model.add(tf.keras.layers.Dense(units=unit/4, activation=\"relu\"))\n",
    "# nn_model.add(tf.keras.layers.Dense(units=unit/6, activation=\"relu\"))\n",
    "\n",
    "#Output Layer\n",
    "nn_model.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summary of Neural Network\n",
    "nn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile the model\n",
    "nn_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "#Train model\n",
    "fit_model = nn_model.fit(X_train, y_train, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model_loss, model_accuracy = nn_model.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Old copy of using one hot encoding on teams/opp teams\n",
    "model_loss, model_accuracy = nn_model.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict = X.loc[(X[\"Team_SAC\"] == 1) & (X[\"Opp_LAC\"] == 1)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "# Fit the StandardScaler\n",
    "X_scaler_test = scaler.fit(test_predict)\n",
    "\n",
    "# Scale the data\n",
    "X_predict = X_scaler_test.transform(X_train)\n",
    "X_predict_test = X_scaler_test.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model.predict(X_predict).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(nn_model.predict(X_predict, batch_size=128).round()).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Model to Predict Point Differential\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_points_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_points = for_points_df['Pts Difference']\n",
    "X_points = for_points_df.drop(['Pts Difference', \"Team\", \"Opp\"], axis=1)\n",
    "X_points = pd.get_dummies(X_points, columns=[\"Home/Away\"])\n",
    "\n",
    "X_train_p, X_test_p, y_train_p, y_test_p = train_test_split(X_points,y_points, random_state=42)\n",
    "\n",
    "# Create a StandardScaler instances\n",
    "scaler_p = StandardScaler()\n",
    "\n",
    "# Fit the StandardScaler\n",
    "X_scaler_p = scaler_p.fit(X_train_p)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled_p = X_scaler_p.transform(X_train_p)\n",
    "X_test_scaled_p = X_scaler_p.transform(X_test_p)\n",
    "\n",
    "\n",
    "#Instantiate Neural Network Model\n",
    "points_model = tf.keras.models.Sequential()\n",
    "\n",
    "unit = X_train_scaled_p.shape[1]\n",
    "\n",
    "#Dense Layers\n",
    "points_model.add(tf.keras.layers.Dense(units=unit, activation=\"relu\", input_dim=X_train_scaled_p.shape[1]))\n",
    "points_model.add(tf.keras.layers.Dense(units=unit, activation=\"relu\"))\n",
    "\n",
    "#Output Layer\n",
    "points_model.add(tf.keras.layers.Dense(units=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summary of Neural Network\n",
    "points_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile the model\n",
    "points_model.compile(loss=\"mean_squared_error\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "#Train model\n",
    "fit_model_p = points_model.fit(X_train_p, y_train_p, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loss_p, model_accuracy_p = points_model.evaluate(X_test_p,y_test_p,verbose=2)\n",
    "print(f\"Loss: {model_loss_p}, Accuracy: {model_accuracy_p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
